{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo notebook shows how to transform data from different file types(.ms, .catalog and StationXML) into parquets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from obspy import read\n",
    "from obspy import read_inventory\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from obspy import read\n",
    "import concurrent.futures\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring access to s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3=boto3.resource('s3',config=Config(signature_version=UNSIGNED))\n",
    "BUCKET_NAME = 'scedc-pds'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following helper function will list all files in specified folder in s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_directories(bucket_name, folder_name, max_directories):\n",
    "    # max_directories is the maximum number of files we want to list\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    directories = []\n",
    "    prefix = folder_name if folder_name.endswith('/') else folder_name + '/'\n",
    "    for obj in bucket.objects.filter(Prefix=prefix, Delimiter='/'):\n",
    "        directory = obj.key\n",
    "        # print(obj.key)\n",
    "        directories.append(directory)\n",
    "\n",
    "        if len(directories) >= max_directories:\n",
    "            return directories[:max_directories]\n",
    "        \n",
    "    return directories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following helper function will download all files from specified folder of s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(bucket_name, directory_list):\n",
    "    for obj in directory_list:\n",
    "        folder_path = '/'.join(obj.split('/')[:-1])\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        s3.Bucket(bucket_name).download_file(obj,obj)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Continous Waveforms (.ms files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We list 10 files from an arbitrary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first five directories: \n",
      "continuous_waveforms/2020/2020_300/AZBZN__BHE___2020300.ms\n",
      "continuous_waveforms/2020/2020_300/AZBZN__BHN___2020300.ms\n",
      "continuous_waveforms/2020/2020_300/AZBZN__BHZ___2020300.ms\n",
      "continuous_waveforms/2020/2020_300/AZBZN__HHE___2020300.ms\n",
      "continuous_waveforms/2020/2020_300/AZBZN__HHN___2020300.ms\n"
     ]
    }
   ],
   "source": [
    "continous_waveform = list_s3_directories(bucket_name = 'scedc-pds', folder_name='continuous_waveforms/2020/2020_300', max_directories=10)\n",
    "\n",
    "# Printing the directory list\n",
    "print('first five directories: ')\n",
    "for directory_name in continous_waveform[0:5]:\n",
    "    print(directory_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download these 10 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_files(BUCKET_NAME, continous_waveform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we convert the .ms files and save them as parquet file. Schemas are yet to be properly configured due to which some data will not appear in final parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the Spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"station\", StringType(), nullable=False),\n",
    "    StructField(\"starttime\", StringType(), nullable=False),\n",
    "    StructField(\"sampling_rate\", StringType(), nullable=False),\n",
    "])\n",
    "\n",
    "# Function to process a single MS file\n",
    "def process_ms_file(ms_file, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "    # Initialize SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MS to Parquet Conversion\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read the MS file using ObsPy\n",
    "    stream = read(ms_file)\n",
    "\n",
    "    # Extract data from ObsPy Stream\n",
    "    data = [(tr.stats.station, str(tr.stats.starttime), str(tr.stats.sampling_rate)) for tr in stream]\n",
    "\n",
    "    # Create Spark DataFrame with the defined schema\n",
    "    spark_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    # Write the Spark DataFrame to Parquet format\n",
    "    output_path = ms_file.replace(\".ms\", \".parquet\")\n",
    "    output_path = folder_path + output_path\n",
    "    spark_df.write.parquet(output_path)\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MS to Parquet Conversion\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# List of MS file paths\n",
    "ms_files = continous_waveform\n",
    "\n",
    "# Create a thread pool executor\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "# Process each MS file in parallel\n",
    "results = []\n",
    "for ms_file in ms_files:\n",
    "    future = executor.submit(process_ms_file, ms_file, 'pyspark/')\n",
    "    results.append(future)\n",
    "\n",
    "# Wait for all tasks to complete\n",
    "concurrent.futures.wait(results)\n",
    "\n",
    "# Shutdown the thread pool executor\n",
    "executor.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all the converted parquet files and combine them to a single parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Directory containing the Parquet files\n",
    "directory = \"pyspark/continuous_waveforms/2020/2020_300\"\n",
    "\n",
    "# Read all Parquet files in the directory into separate DataFrames\n",
    "parquet_files = [file for file in os.listdir(directory) if file.endswith(\".parquet\")]\n",
    "dataframes = [spark.read.parquet(os.path.join(directory, file)) for file in parquet_files]\n",
    "\n",
    "# Perform a union of the DataFrames\n",
    "combined_df = dataframes[0]  # Take the first DataFrame as the starting point\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "# Write the combined DataFrame to a single Parquet file\n",
    "combined_df.write.parquet(\"output/combined.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the final parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------+-------------+\n",
      "|station|starttime                  |sampling_rate|\n",
      "+-------+---------------------------+-------------+\n",
      "|BZN    |2020-10-26T02:49:58.788400Z|100.0        |\n",
      "|BZN    |2020-10-26T02:49:58.008400Z|100.0        |\n",
      "|BZN    |2020-10-26T00:00:00.008300Z|100.0        |\n",
      "|BZN    |2020-10-26T00:00:00.008300Z|100.0        |\n",
      "|BZN    |2020-10-26T00:00:00.008300Z|100.0        |\n",
      "|BZN    |2020-10-26T02:49:58.008400Z|100.0        |\n",
      "|BZN    |2020-10-26T00:00:00.008300Z|100.0        |\n",
      "|BZN    |2020-10-26T00:00:00.008300Z|100.0        |\n",
      "|BZN    |2020-10-26T00:00:00.008300Z|100.0        |\n",
      "|BZN    |2020-10-26T00:00:00.019500Z|40.0         |\n",
      "|BZN    |2020-10-26T00:00:00.019500Z|40.0         |\n",
      "|BZN    |2020-10-26T00:00:00.019500Z|40.0         |\n",
      "|BZN    |2020-10-26T00:00:00.069500Z|1.0          |\n",
      "+-------+---------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into a Spark DataFrame\n",
    "parquet_df = spark.read.parquet(\"output/combined.parquet\")\n",
    "\n",
    "# Display the contents of the Spark DataFrame\n",
    "parquet_df.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Earthquake .Catalog files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We list 10 files from an arbitrary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first five directories: \n",
      "earthquake_catalogs/SCEC_DC/1932.catalog\n",
      "earthquake_catalogs/SCEC_DC/1933.catalog\n",
      "earthquake_catalogs/SCEC_DC/1934.catalog\n",
      "earthquake_catalogs/SCEC_DC/1935.catalog\n",
      "earthquake_catalogs/SCEC_DC/1936.catalog\n"
     ]
    }
   ],
   "source": [
    "earthquake_catalog = list_s3_directories(bucket_name = 'scedc-pds', folder_name='earthquake_catalogs/SCEC_DC', max_directories=10)\n",
    "\n",
    "# Printing the directory list\n",
    "print('first five directories: ')\n",
    "for directory_name in earthquake_catalog[0:5]:\n",
    "    print(directory_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_files(BUCKET_NAME, earthquake_catalog)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we convert the .catalog files and save them as parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file paths\n",
    "file_paths = earthquake_catalog\n",
    "\n",
    "\n",
    "output_dir = 'earthquake_catalogs_parquet'\n",
    "\n",
    "# Function to process a catalog file\n",
    "def process_catalog_file(file_path):\n",
    "    catalog_file = spark.read.text(file_path)\n",
    "    \n",
    "    # Filter out the header and the rows until the line with \"###\"\n",
    "    filtered_catalog_file = catalog_file.filter(~catalog_file['value'].startswith(\"#\")) \\\n",
    "        .filter(~catalog_file['value'].startswith(\"###\"))\n",
    "\n",
    "    # Split the lines of the filtered catalog file into columns using regular expressions\n",
    "    catalog_df = filtered_catalog_file.select(split(filtered_catalog_file['value'], '\\\\s+').alias('columns'))\n",
    "\n",
    "    # Extract columns from the columns array\n",
    "    catalog_df = catalog_df.select(\n",
    "        col(\"columns\").getItem(0).alias(\"Date\"),\n",
    "        col(\"columns\").getItem(1).alias(\"Time\"),\n",
    "        col(\"columns\").getItem(2).alias(\"ET\"),\n",
    "        col(\"columns\").getItem(3).alias(\"GT\"),\n",
    "        col(\"columns\").getItem(4).alias(\"MAG\"),\n",
    "        col(\"columns\").getItem(5).alias(\"M\"),\n",
    "        col(\"columns\").getItem(6).alias(\"LAT\"),\n",
    "        col(\"columns\").getItem(7).alias(\"LON\"),\n",
    "        col(\"columns\").getItem(8).alias(\"DEPTH\"),\n",
    "        col(\"columns\").getItem(9).alias(\"Q\"),\n",
    "        col(\"columns\").getItem(10).alias(\"EVID\"),\n",
    "        col(\"columns\").getItem(11).alias(\"NPH\"),\n",
    "        col(\"columns\").getItem(12).alias(\"NGRM\")\n",
    "    )\n",
    "    # Save the DataFrame as Parquet\n",
    "    catalog_df.write.parquet(output_dir + '/' + file_path.split('/')[-1].split('.')[0] + '.parquet')\n",
    "\n",
    "    return catalog_df\n",
    "\n",
    "# Create a ThreadPoolExecutor\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "# Process catalog files in parallel\n",
    "catalog_dfs = list(executor.map(process_catalog_file, file_paths))\n",
    "\n",
    "# Wait for all tasks to complete\n",
    "concurrent.futures.wait(results)\n",
    "\n",
    "# Shutdown the thread pool executor\n",
    "executor.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine all these converted parquets into single parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the Parquet files\n",
    "directory = \"earthquake_catalogs_parquet\"\n",
    "\n",
    "# Read all Parquet files in the directory into separate DataFrames\n",
    "parquet_files = [file for file in os.listdir(directory) if file.endswith(\".parquet\")]\n",
    "dataframes = [spark.read.parquet(os.path.join(directory, file)) for file in parquet_files]\n",
    "\n",
    "# Perform a union of the DataFrames\n",
    "combined_df = dataframes[0]  # Take the first DataFrame as the starting point\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "# Write the combined DataFrame to a single Parquet file\n",
    "combined_df.write.parquet(\"output/combined_earthquake_catalogs.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising that parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---+---+----+---+------+--------+-----+---+-------+---+----+\n",
      "|Date      |Time       |ET |GT |MAG |M  |LAT   |LON     |DEPTH|Q  |EVID   |NPH|NGRM|\n",
      "+----------+-----------+---+---+----+---+------+--------+-----+---+-------+---+----+\n",
      "|1933/01/01|13:31:30.27|eq |l  |2.26|l  |33.820|-117.985|6.0  |D  |3359497|6  |0   |\n",
      "|1933/01/01|22:45:24.12|eq |l  |2.09|l  |33.831|-118.145|6.0  |C  |3359499|5  |0   |\n",
      "|1933/01/03|04:45:39.15|eq |l  |2.34|h  |33.604|-116.768|6.0  |D  |3359503|4  |0   |\n",
      "|1933/01/03|12:39:12.21|eq |l  |2.12|h  |33.912|-116.960|6.0  |D  |3359504|4  |0   |\n",
      "|1933/01/03|22:40:26.95|eq |l  |1.38|h  |34.404|-118.371|6.0  |D  |3361946|5  |0   |\n",
      "|1933/01/04|06:36:14.60|eq |l  |2.39|l  |33.352|-116.832|6.0  |D  |3359508|6  |0   |\n",
      "|1933/01/05|09:31:17.44|eq |l  |2.58|l  |33.895|-118.994|10.0 |D  |3359512|6  |0   |\n",
      "|1933/01/05|23:29:19.48|qb |l  |1.55|h  |34.260|-117.830|0.0  |D  |3359516|5  |0   |\n",
      "|1933/01/07|05:56:44.00|eq |l  |2.32|h  |33.805|-119.714|6.0  |C  |3359519|4  |0   |\n",
      "|1933/01/07|08:12:05.36|eq |l  |3.01|l  |33.700|-119.669|6.0  |C  |3359518|6  |0   |\n",
      "|1933/01/07|14:13:19.70|eq |l  |2.81|l  |33.161|-116.199|6.0  |C  |3359521|8  |0   |\n",
      "|1933/01/08|08:18:27.45|eq |l  |3.22|l  |34.176|-120.771|6.0  |C  |3359530|2  |0   |\n",
      "|1933/01/08|19:28:25.43|eq |l  |3.45|l  |33.288|-115.968|6.0  |C  |3359531|1  |0   |\n",
      "|1933/01/12|05:38:11.52|eq |l  |1.87|l  |34.384|-117.468|6.0  |C  |3359545|2  |0   |\n",
      "|1933/01/12|10:15:19.42|eq |l  |1.70|h  |34.129|-117.440|6.0  |C  |3359546|2  |0   |\n",
      "|1933/01/13|10:14:41.03|eq |l  |3.04|l  |34.405|-117.218|6.0  |C  |3359547|2  |0   |\n",
      "|1933/01/16|08:25:39.44|eq |l  |2.55|l  |34.331|-117.126|6.0  |C  |3359549|2  |0   |\n",
      "|1933/01/16|11:21:30.53|eq |l  |2.19|h  |34.328|-117.114|6.0  |C  |3359550|1  |0   |\n",
      "|1933/01/16|23:40:58.07|eq |l  |2.70|l  |34.242|-117.171|6.0  |C  |3359551|2  |0   |\n",
      "|1933/01/18|18:22:28.22|eq |l  |2.10|h  |34.214|-117.246|6.0  |C  |3359555|1  |0   |\n",
      "+----------+-----------+---+---+----+---+------+--------+-----+---+-------+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into a Spark DataFrame\n",
    "parquet_df = spark.read.parquet(\"output/combined_earthquake_catalogs.parquet\")\n",
    "\n",
    "# Display the contents of the Spark DataFrame\n",
    "parquet_df.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing FDSN StationXML files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download an arbitarty StationXML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY=\"FDSNstationXML/CI/CI_GSC.xml\"\n",
    "s3.Bucket(BUCKET_NAME).download_file(KEY,'CI_GSC.xml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualise content of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationxml_file_path = 'CI_GSC.xml'\n",
    "inventory = read_inventory(stationxml_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inventory created at 2022-08-11T16:46:02.326506Z\n",
       "\tSending institution: ANSS Station Information System (ANSS Station Information System)\n",
       "\tContains:\n",
       "\t\tNetworks (1):\n",
       "\t\t\tCI\n",
       "\t\tStations (1):\n",
       "\t\t\tCI.GSC (Goldstone)\n",
       "\t\tChannels (485):\n",
       "\t\t\tCI.GSC..ABT, CI.GSC..ACE (7x), CI.GSC..ACA, CI.GSC..ACF, \n",
       "\t\t\tCI.GSC..ACK, CI.GSC..ACQ, CI.GSC..ACS, CI.GSC..ADG, CI.GSC..ADL, \n",
       "\t\t\tCI.GSC..ADT, CI.GSC..ALL, CI.GSC..AMD, CI.GSC..ANI, CI.GSC..APK, \n",
       "\t\t\tCI.GSC..APO, CI.GSC..ARD, CI.GSC..ASL, CI.GSC..ASQ, CI.GSC..ATH, \n",
       "\t\t\tCI.GSC..AWR, CI.GSC..BCI (3x), CI.GSC..BHZ (15x), CI.GSC..BHN (15x)\n",
       "\t\t\tCI.GSC..BHE (16x), CI.GSC..EHZ (2x), CI.GSC..ELZ, \n",
       "\t\t\tCI.GSC..HCI (3x), CI.GSC..HDI, CI.GSC..HDO, CI.GSC..HHZ (14x), \n",
       "\t\t\tCI.GSC..HHN (14x), CI.GSC..HHE (15x), CI.GSC..HLZ (8x), \n",
       "\t\t\tCI.GSC..HLN (8x), CI.GSC..HLE (8x), CI.GSC..HNZ (10x), \n",
       "\t\t\tCI.GSC..HNN (10x), CI.GSC..HNE (10x), CI.GSC..LAT, CI.GSC..LCE (7x)\n",
       "\t\t\tCI.GSC..LCL (7x), CI.GSC..LCQ (7x), CI.GSC..LDZ (3x), \n",
       "\t\t\tCI.GSC..LDN (3x), CI.GSC..LDE (3x), CI.GSC..LDI, CI.GSC..LDO (6x), \n",
       "\t\t\tCI.GSC..LEA, CI.GSC..LEB, CI.GSC..LEC, CI.GSC..LED, CI.GSC..LEM, \n",
       "\t\t\tCI.GSC..LEP, CI.GSC..LES, CI.GSC..LEV, CI.GSC..LFT, \n",
       "\t\t\tCI.GSC..LHZ (14x), CI.GSC..LHN (14x), CI.GSC..LHE (15x), \n",
       "\t\t\tCI.GSC..LLZ (7x), CI.GSC..LLN (7x), CI.GSC..LLE (7x), \n",
       "\t\t\tCI.GSC..LNZ (10x), CI.GSC..LNN (10x), CI.GSC..LNE (10x), \n",
       "\t\t\tCI.GSC..LON, CI.GSC..LOG (7x), CI.GSC..LSU, CI.GSC..OCF (7x), \n",
       "\t\t\tCI.GSC..VAX, CI.GSC..VCE (6x), CI.GSC..VCA, CI.GSC..VCB, \n",
       "\t\t\tCI.GSC..VCO (7x), CI.GSC..VCQ (6x), CI.GSC..VDO, CI.GSC..VEN, \n",
       "\t\t\tCI.GSC..VEA (7x), CI.GSC..VEC (7x), CI.GSC..VEI, CI.GSC..VEO, \n",
       "\t\t\tCI.GSC..VEP (7x), CI.GSC..VEU, CI.GSC..VFP (6x), CI.GSC..VG1, \n",
       "\t\t\tCI.GSC..VG2, CI.GSC..VHZ (13x), CI.GSC..VHN (13x), \n",
       "\t\t\tCI.GSC..VHE (14x), CI.GSC..VI1, CI.GSC..VKI (7x), CI.GSC..VMZ (13x)\n",
       "\t\t\tCI.GSC..VMN (13x), CI.GSC..VME (14x), CI.GSC..VPB, CI.GSC.2C.HNZ\n",
       "\t\t\tCI.GSC.2C.HNN, CI.GSC.2C.HNE"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inventory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file mostly contains metadata. We take tabular data from it and make a parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = inventory.networks[0].stations\n",
    "# Access channel information for the station\n",
    "channels = stations[0].channels\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Channel\", StringType(), nullable=False),\n",
    "    StructField(\"StartTime\", StringType(), nullable=False),\n",
    "    StructField(\"EndTime\", StringType(), nullable=True),\n",
    "    StructField(\"ChannelTypes\", StringType(), nullable=False),\n",
    "    StructField(\"SensorDescription\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Define a function to extract the relevant information from the channel data\n",
    "def extract_channel_info(channel):\n",
    "    channel_data = {\n",
    "        \"Channel\": channel.code,\n",
    "        \"StartTime\": channel.start_date,\n",
    "        \"EndTime\": channel.end_date,\n",
    "        \"ChannelTypes\": \", \".join(channel.types),\n",
    "        \"SensorDescription\": channel.sensor.description if channel.sensor else None\n",
    "    }\n",
    "    return channel_data\n",
    "\n",
    "# Create a list of dictionaries representing the channel data\n",
    "channels_data = [extract_channel_info(channel) for channel in channels]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = spark.createDataFrame(channels_data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.write.parquet(\"output/FDSNXML.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "|Channel|StartTime                                                                                                                         |EndTime                                                                                                                           |ChannelTypes      |SensorDescription |\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "|ABT    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Reboot-count      |\n",
      "|ACA    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Comm-attempt-count|\n",
      "|ACE    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1260984000000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|{_UTCDateTime__precision=6, _UTCDateTime__ns=1433278800000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|CONTINUOUS, HEALTH|Clock Status      |\n",
      "|ACE    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1433278800000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|{_UTCDateTime__precision=6, _UTCDateTime__ns=1435104000000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|CONTINUOUS, HEALTH|Clock Status      |\n",
      "|ACE    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1435104000000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|{_UTCDateTime__precision=6, _UTCDateTime__ns=1449595800000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|CONTINUOUS, HEALTH|Clock Status      |\n",
      "|ACE    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1449595800000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|{_UTCDateTime__precision=6, _UTCDateTime__ns=1506616200000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|CONTINUOUS, HEALTH|Clock Status      |\n",
      "|ACE    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1506616200000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|{_UTCDateTime__precision=6, _UTCDateTime__ns=1637269200000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|CONTINUOUS, HEALTH|Clock Status      |\n",
      "|ACE    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1637269200000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|CONTINUOUS, HEALTH|Clock Status      |\n",
      "|ACE    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Clock Status      |\n",
      "|ACF    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Comm-efficiency   |\n",
      "|ACK    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Chksum-errors     |\n",
      "|ACQ    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Calibration-input |\n",
      "|ACS    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Comm-ok-count     |\n",
      "|ADG    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Data-gap-count    |\n",
      "|ADL    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Data-latency      |\n",
      "|ADT    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Comm-duty-cycle   |\n",
      "|ALL    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|LL-data-latency   |\n",
      "|AMD    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Data-missing      |\n",
      "|ANI    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Ip-change-count   |\n",
      "|APK    |{_UTCDateTime__precision=6, _UTCDateTime__ns=1660154400000000000, __class__=obspy.core.utcdatetime.UTCDateTime, _initialized=true}|null                                                                                                                              |CONTINUOUS, HEALTH|Packet-count      |\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file into a Spark DataFrame\n",
    "parquet_df = spark.read.parquet(\"output/FDSNXML.parquet\")\n",
    "\n",
    "# Display the contents of the Spark DataFrame\n",
    "parquet_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
